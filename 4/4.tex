\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{../common}
\usepackage{../pagesetup}
\usepackage{xcolor}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:
\renewcommand{\v}{\boldsymbol}
\newcommand{\argmax}{arg\,max}
\begin{document}
\newcommand{\Norm}{{\mathcal{N}}}
\newcommand{\trace}{\on{tr}}
\lecture{4}{September 13}{Sasha Rush}{Kojin Oshiba, Michael Ge, Aditya Prasad, Scott Sussex}{Linear Regression}

\subsection{Multivariate Normal (MVN)}
The multivariate normal distribution of a $D$-dimensional random vector X is defined as:

$$\Norm(X; \mu,\Sigma) \sim (2\pi)^{-\frac{D}{2}} |\Sigma|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}(X-\mu)^T\Sigma^{-1} (X-\mu)\right)$$

Note: \begin{itemize}
\item $(2\pi)^{-\frac{D}{2}} |\Sigma|^{-\frac{1}{2}}$ and $-\frac{1}{2}$ are constants we can ignore in MLE and MAP calculations for $\mu$. 
\item $(X-\mu)^T\Sigma^{-1} (X-\mu)$ is a quadratic term.
\end{itemize}

\noindent Given a set of $X$ values, there are two types of inference on $\mu$ and $\Sigma$ that we're interested in doing: MLE and MAP. We are also interested in doing predictions on future $X$ values.

\subsection{Maximum Likelihood of MVN} 
Let $\theta = (\mu, \Sigma)$, where $\Sigma$ can be approximated as a diagonal/low rank matrix. If there are $x_1, \ldots, x_n$ observations, the MLE estimate of $\mu$ is 
\begin{align*}
\mu^* & = \underset{\mu}{\arg\max} -\sum_{i=0}^{n}\log \N(x_i;\mu,\Sigma) & \\
	  & = \underset{\mu}{\arg\max} \quad \log(constant) - \sum_{i=0}^{n}(x_i-\mu)^T\Sigma^{-1} (x_i-\mu) & \\
      & = \underset{\mu}{\arg\max} - \sum_{i=0}^{n}(x_i-\mu)^T\Sigma^{-1} (x_i-\mu)
\end{align*}

\noindent Let $L = \sum_{i=0}^{n}(x_i-\mu)^T\Sigma^{-1} (x_i-\mu)$.

\begin{align*}
\frac{dL}{d\mu} = \Sigma_{n} \Sigma^{-1} (x_n-\mu) &= 0 & \\
\Leftrightarrow \mu^*_{MLE} & = \frac{\Sigma_{X_n}}{N} 
\end{align*}

\noindent Similarly,

\begin{align*}
\frac{dL}{d\Sigma} = (exercise) & = 0 & \\
\Leftrightarrow \Sigma^*_{MLE} & = \frac{1}{N} \sum_{i=0}^{n} x_i x_i^T = \frac{1}{N} X^\top X
\end{align*}

\begin{exercise}
Calculate $\frac{dL}{d\Sigma}$ The following might be helpful: 
\begin{itemize}
\item $\frac{d}{dA} ln |A| = A^{-1}$ \\
\item $\frac{d}{dA} tr(BA) = B^T$ \\
\item $\trace(ABC) = \trace(BCA)$
\end{itemize}
\end{exercise}

\begin{proof}
(Solution also in Murphy page 102) \\
Define $\Delta = \Sigma^{-1}$ as the precision matrix. Let 
\begin{align*}
L &= \frac{N}{2} \log(|\Delta|) - \frac{1}{2} \sum_{i=0}^{n}(x_i-\mu)^T \Delta (x_i-\mu) \\
&= \frac{N}{2} \log(|\Delta|) - \frac{1}{2} \sum_{i=0}^{n} \trace((x_i-\mu)^T (x_i-\mu) \Delta
\end{align*}

Now 

$$\frac{dL}{d\Gamma} = \frac{N}{2} \Delta^{T} - \frac{1}{2} X^T X  $$


$\Delta$ is orthogonal so $\Delta^T = \Delta^{-1} = \Sigma$. This gives the result

$$\frac{dL}{d\Sigma} = \frac{N}{2} \Sigma - \frac{1}{2} X^T X   $$
\end{proof}



 
\subsection{Linear-Gaussian Models}

Let $x$ be a vector of affine, noisy observations with a prior distribution: $$x \sim N(m_0, S_0)$$ Let $y$ be the outputs: $$y|x \sim \Norm(Ax + b, \Sigma_y)$$

\subsubsection{$p(x|y)$}
We are interested in calculating the posterior distribution: $p(x|y)$.
\begin{align*}
  p(x|y) & \propto p(x) p(y|x) \\
         & =
           \frac{1}{2} \exp \begin{cases}
             (x - m_0)^\top S_0^{-1} (x - m_0) \\
             + (y - (Ax + b))^\top \Sigma_y^{-1} (y - (Ax + b)) \\
           \end{cases} \\
         & = \frac{1}{2} \exp
           \begin{cases} 
             \underline{x^\top S_0^{-1} x}^{\star \star} - \underline{2x^\top S_0^{-1} m_0}^{\star} + \ldots \\
             + \underline{x^\top (A^\top \Sigma_y^{-1}A)x}^{\star \star} - \underline{2x^\top(A^\top\Sigma_y^{-1})y}^{\star} + \underline{2x^\top(A^\top\Sigma_y^{-1})b}^{\star} + \ldots
           \end{cases}
\end{align*}

The terms containing $x$ are underlined. Double-starred ($\star \star$) terms are quadratic in $x$, while single-starred ($\star$) terms are linear in $x$. The remaining terms are constants that are swallowed up by the proportionality. By Gaussian-Gaussian conjugacy, we know the resulting distribution should be Gaussian. To find the parameters, we'll modify $p(x|y)$ to fit the form of a Normal. This requires completing the square!

\subsubsection{Completing the Square}

$$ax^2 + bx + c \to a(x - h)^2 + k, h = \frac{-b}{2a}, k = c - \frac{b^2}{4a}$$

We ignore the $k$ term since it too is swallowed up in the proportionality. In application to our problem, we group the quadratic and linear terms together to calculate our terms for completing the square.

\begin{itemize}
\item ``a'' is $S_N^{-1} = S_0^{-1} + A^\top \Sigma_y^{-1}A$
\item ``h'' is $m_N = S_N \left[S_0^{-1}m_0 + A^\top \Sigma_y^{-1}(y - b)\right]$
\end{itemize}

In this more ``intuitive'' representation, we find that $p(x|y)$ has the form of $\Norm(m_N, S_N)$. Murphy also has a more explicit representation:

\begin{itemize}
\item $\Sigma_{x|y} = \Sigma_x^{-1} + A^\top \Sigma_y^{-1}A$
\item $\mu_{x|y} = \Sigma_{x|y} [\Sigma_x^{-1} \mu_x + A^\top \Sigma_x^{-1}(y - b)]$
\end{itemize}

\subsubsection{p(y)}

We now calculate the normalizer term, $p(y)$. Now, $x$ is fixed. $y$ follows the linear model:

$$y = Ax + b + \epsilon$$

The result is that $y$ follows a Normal distribution with the following form:
$$p(y) = \Norm(y;Am_0 + b, \Sigma_y + A \Sigma_x A^\top)$$

\subsubsection{Prior (just for $\mu$)}
$$p(\mu) = \Norm(\mu|m_0,S_0)$$
where $m_0$, $S_0$ are prior mean, prior variance. $p(\mu)$ is defined Gaussian because Gaussian is the conjugate prior of itself. A prior is called a conjugate prior if it has the same distribution as the posterior distribution.

\subsubsection{Posterior (just for $\mu$)}
$$p(\mu|X) \propto p(\mu)p(X|\mu) = \Norm(\mu|m_0,s_0)\Norm(X;\mu,\Sigma)$$
This is a special case of linear regression. Recall,
\begin{itemize}
\item ``a'' is $S_N^{-1} = S_0^{-1} + A^\top \Sigma_y^{-1}A$
\item ``h'' is $m_N = S_N \left[S_0^{-1}m_0 + A^\top \Sigma_y^{-1}(y - b)\right]$
\end{itemize}
We let $b=0$ and $A=I$. Then we obtain,
$$S_N^{-1} = S_0^{-1} + \Sigma^{-1}$$
$$m_N = S_N[S_0^{-1}m_0+\Sigma^{-1}X]$$
Hence,
$$p(\mu|X) = \Norm(m;m_N,S_N)$$

\subsubsection{Unknown Variance}
Similar to $\mu$, we can also define a conjugate prior on $\Sigma$, which is Inverse Wishart distribution. It is defined as:
$$IW(\Sigma|S,\nu)=\frac{1}{2}|\Sigma|^{-(\nu-(D+1)/2)}\exp\{\frac{1}{2}tr(S^{-1}\Sigma^{-1})\}$$

\begin{itemize}
\item distribution over positive semi definite $\Sigma$ with two parameters $S,\nu$.
\item $S=\Sigma XX^T$ is a prior scatter matrix called the scale matrix. $\nu=n\mu$ is degrees of freedom where $\nu - (D+1)$ is the number of observations.
\end{itemize}

\subsection{Linear Regression}
In an undergraduate version of the class, we might define the problem as follows: We are given ``fixed'' set of inputs, $\left \{ \v{x}_i \right\}$. We want to ``predict" the outputs.
\\

\noindent Here, we define the problem as attempting to compute $p(y \given \v{x}, \v{\theta})$. Consider the following example. We assume that our data is generated as follows:
	\begin{align*}
		y = \v{w}^T \v{x} + \text{noise}
	\end{align*} 
	
\noindent Further, we assume that the noise (denoted by $\epsilon$) is distributed as Gaussian with mean $0$; that is:
	\begin{align*}
		\epsilon \sim \mathcal{N}(0, \sigma^2)
	\end{align*}
	
\noindent Then, we have:
	\begin{align*}
		p(y \given \v{x}, \theta) = \Norm (y \given \v{w^T}\v{x}, \sigma^2)
	\end{align*}
\noindent Note that the bias term here is included as a dimension in $\v{w}$, $w_0$.

\subsubsection{Log Likelihood}
Consider a data-set that looks like $\left \{ (\v{x_i}, y_i)\right\}_{i=1}^{N}$. The log-likelihood $\mathcal{L}(\v{\theta})$ is given by:
	\begin{align*}
		\mathcal{L}(\v{\theta}) &= \log p(\text{data} \given \v{\theta}) \\
							&= \sum_{n=1}^N \log p (y_n \given \v{x_n}, \v{\theta})\\
							&= \sum_{n=1}^N \log(\text{constant}) - \frac{1}{2 \sigma^2} (y_n - \v{w}^T \v{x_n})^2
	\end{align*}
	Note that data here refers to just the $y_i$'s. The $y_n$'s are called the target; the $\v{w}$ represents the weights; and the $x_n$'s are the observations. The term $(y_n - \v{w}^T \v{x_n})^2$ is essentially just the residual sum of squares.
	
	\subsubsection{Computing MLE}
	We want the argmax of the log-likelihood. We therefore have:
		\begin{align*}
			\text{argmax}_{\v{w}} \mathcal{L}(\v{w}) &= \text{argmax} - \sum_{n=1}^N \frac{1}{2 \sigma^2} (y_n - \v{w}^T \v{x_n})^2 \\
			&= \text{argmax}_{\v{w}} -  \left[ \v{y} - \v{X} \v{w} \right]^T \left[ \v{y} - \v{X} \v{w} \right] \\
			&= \text{argmax}_{\v{w}} \left[ \v{w}^T \v{X^T} \v{X} \v{w} - 2 \v{w}^T \v{X^T} \v{y} + \text{constant} \right]			
		\end{align*}
		
		There is an analytical solution to this, and we obtain it by simply computing the gradient and setting it to $\v{0}$. 
		\begin{align*}
			\partial_{\v{w}} \left [\v{w}^T \v{X^T} \v{X} \v{w} - 2 \v{w}^T \v{X^T} \v{y} \right] = 2 \v{X^T} \v{X} \v{w} - 2 \v{X^T} \v{y}
		\end{align*}
		Setting this to $\v{0}$, we obtain:
		\begin{align*}
			\v{w}_{MLE} = (\v{X}^T \v{X})^{-1} \v{X}^T \v{y}
		\end{align*}			
		
		As we will see in homework $1$, $(\v{X}^T \v{X})^{-1} \v{X}^T \v{y}$ can be viewed as the projection of $\v{y}$ onto the column space of $\v{X}$.

\subsection{Bayesian Linear Regression}
In the Bayesian framework, we also introduce a probability distribution on the weights. Here, we choose:
	\begin{align*}
		p(\v{w}) = \Norm(\v{w} \given \v{m_0}, \v{S_0}) 
	\end{align*}
	
	Thus, we have:
	\begin{align*}
		p(\v{y} \given \v{X}, \v{w}, \v{\mu}, \sigma^2) = \Norm(\v{y} \given \v{\mu} + \v{X^T} \v{w}, \sigma^2 \v{I})
	\end{align*}
	We assume that $\v{\mu} = 0$.
	
The posterior then is of the form:
\begin{align*}
	p(\v{w} \given \ldots) \propto \Norm(\v{w} \given \v{m_0}, \v{S_0}) \Norm(\v{y} \given  \v{X}^T \v{w}, \sigma^2 \v{I})
\end{align*}

Applying the results obtained above with the linear Gaussian results, with:
	\begin{align*}
		b = 0 \\
		\v{A} = \v{X}^T \\
		\v{\Sigma_y} = \sigma^2 \v{I}
	\end{align*}
	Thus, we have:
	\begin{align*}
		\v{S_N}^{-1} = \v{S_0}^{-1} + \frac{1}{\sigma^2}\v{X^T} \v{X} \\
		\v{m_N} = \v{S_N} \left [\v{S_0^{-1}} \v{m_0} + \v{X^T} \v{y} \frac{1}{\sigma^2} \right]
	\end{align*}
	
	Now, we compute the posterior predictive:
	\begin{align*}
		p(y^{~} \given \v{x}, y) = \int \Norm(y^{~}\given \v{w}^T \v{x}, \sigma^2) \Norm(\v{w} \given \v{m_N}, \v{S_N}) d\v{w}
	\end{align*}
	
	Using the form for the marginal derived earlier, we have:
	\begin{align*}
		p(y^{~} \given \v{x}, y) = \Norm (y^{~} \given \v{X^T} \v{m_N}, \sigma^2 + \v{X^T} \v{S_0} \v{X})
	\end{align*}
	
	The variance term is particularly interesting because now the variance has dependence on the actual data; thus, the Bayesian method has thus produced a different result. The mean, however, is the same as the MAP estimate ($\v{x}^T \v{m_N}$)

\subsection{Non Linear Regression}

All the examples done so far have been in linear space. To define an adaptive basis, we simply transform point $x$ with the transormation of our choice:
$$x \to \phi(x)$$

Examples include:
\begin{itemize}
\item $\phi_1(x) = \sin(x)$
\item $\phi_2(x) = \sin(\lambda x)$
\item $\phi_3(x) = \max(0, x)$
\item $\phi(x; w) = \max(0, w'\top x)$
\end{itemize}
The last example is the core of neural networks and deep learning where the weights are learned for each level of $w$.
\end{document}



